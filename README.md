# IRCTC Realtime ticket booking data sync to Google BigQuery

## Ovierview

What is IRCTC ?  
IRCTC stands for Indian Railway Catering and Tourism Corporation. It's a subsidiary of Indian Railways, operating as the official online portal for booking train tickets, providing catering services, and offering tourism packages. 

This project provides a solution for syncing ticket booking data in real-time to Google BigQuery. It utilizes a combination of Python, Google Cloud Pub/Sub, GCP Dataflow (Apache Beam), and BigQuery to achieve this.


## Directory structure:

rits98-irctc-realtime-ticket-booking-data-sync-to-google-bigquery/  
    ├── README.md  
    ├── bigquery_create_table.sql  
    ├── data_mocker.py  
    └── transform_data.py  

1. bigquery_create_table.sql -> DML query to create table to ingest and store the data.
2. data_mocker.py -> It simulates the generation of data and mimics the IRCTC system.
3. transform_data.py  -> It transforms the data once it is consumed from GCP Pub/Sub before loading it into the BigQuery table.

## Features
	-  Real-time Data Ingestion: Captures ticket booking events as they occur.
	-  Data Pipeline: Utilizes tools like BigQuery, Google Cloud Pub/Sub, and Dataflow for scalable data processing.
	-  BigQuery Integration: Stores processed data in Google BigQuery for fast querying and analytics.
	-  Fault Tolerance: Ensures data consistency and reliability in case of failures.
	-  Extensible Architecture: Easily adaptable for additional data sources or destinations.

## Architecture

![Architecture](https://github.com/user-attachments/assets/604363a3-543d-4089-96c4-9aba98821dd3)

## Data Flow
	1.	Event Generation: Ticket booking events are generated by mock python code to mimic the IRCTC system.
	2.	Streaming Ingestion: Events are published to a message broker (e.g., GCP Pub/Sub).
	3.	Processing: Dataflow jobs transform, clean, and enrich the data.
	4.	Loading: Processed data is written to BigQuery tables.
	5.	Analytics: Data in BigQuery is available for BI tools, dashboards, and ad-hoc queries.

## Steps

###  Enable all service permissions

1.  Go to `IAM & Admin`
2.  Click on `Service Accounts` on the side panel and then click on the service accounts
<br>
<img width="1048" alt="image" src="https://github.com/user-attachments/assets/64f827a5-afd5-4ffa-bc81-a962c8ece4b8" />
<br>
3. Click on the `Permission` option in the tab and then `Manage Access`.
<br>
<img width="1101" alt="image" src="https://github.com/user-attachments/assets/00a288e9-12e1-4655-abed-6db9e1cc901c" />
<br>
4. Add all the necessary roles.
<br>
<img width="1673" alt="image" src="https://github.com/user-attachments/assets/70acb01b-21ab-45fa-bc36-8ef65ebee40b" />
<br>





### Create Topic in GCP Pub/Sub

1. Go to Pub/Sub in GCP
2. Click `Create Topic` option on the tab
<br>
<img width="982" alt="image" src="https://github.com/user-attachments/assets/5fd4272c-6ce9-43ed-bfe3-e9a2d0a1041d" />
<br>
3. Give a meaningful name to the topic. Select the required options below. 
4. If we want we can decide a schema too.

   - A Pub/Sub schema is an optional feature that you can use to enforce the format of the data field in a Pub/Sub message.
   - A schema creates a contract between the publisher and subscriber about the format of the messages. Pub/Sub enforces this format. Schemas facilitate inter-team consumption of data streams in your organization by creating a central authority for message types and permissions. A Pub/Sub message schema defines the names and data types for the fields in a message.
   - You can create a schema and associate it with a topic to enforce the schema for published messages. If a specific message does not conform to the schema, the message is not published. You can also create additional revisions for a schema.

<br>
<img width="1081" alt="image" src="https://github.com/user-attachments/assets/cb019f59-81cc-4813-bb08-35b6f2841141" />
<br>

5. Click on `Create` and also test with a custom message to ensure its working
<br>
<img width="1315" alt="image" src="https://github.com/user-attachments/assets/fe43408a-8d31-431a-9328-5a5499834caf" />
<br>
6. Now create the topic. I have enable message retention option and given 7 days. (For 7 days the message will be present in the GCP Pub/Sub and then it will be deleted irrespective of whether it is consumed or not)
<br>
<img width="890" alt="image" src="https://github.com/user-attachments/assets/b96912d5-bdd8-466c-91f2-fcbfc257a923" />
<br>
8. Select the required schema. Press `Click` button.


### Create table in GCP Bigquery

1. Open Bigquery in GCP
2. Create a data warehouse in the Bigquery to store the tables
<br>
<img width="969" alt="image" src="https://github.com/user-attachments/assets/534550b6-10f1-4aac-895e-2cd605edc733" />
<br>

3. Give a name and choose a location and click on `Create Dataset` button
<br>
<img width="574" alt="image" src="https://github.com/user-attachments/assets/b470b0aa-462b-49f9-8fa2-ac0a20280a2e" />
<br>
4. Create a table in that warehouse.
<br>
<img width="924" alt="image" src="https://github.com/user-attachments/assets/0262553c-99ab-4fee-97a7-820cc1ffc7e2" />
<br>
6. 


### Create a Dataflow Job

1. Create a Job using templete.
<br>
<img width="961" alt="image" src="https://github.com/user-attachments/assets/4b5cad4c-776f-4224-84ab-075a66a163a6" />
<br>
2. Give the necessay details.
<br>
<img width="956" alt="image" src="https://github.com/user-attachments/assets/8008ba4a-d36d-4ef0-b346-f3df6da5499b" />

3. I want to use custom transformation logic for the incoming data. The transfromed data will be sent to the Bigquery table.
4. The transformation code is kept in the `transform_data.py` file inside the `irctc_data_transformation_ritayan` folder of the `bigquery_ritayan` bucket.
<br>
<img width="1161" alt="image" src="https://github.com/user-attachments/assets/2d97fc37-1527-4b90-a93b-9d7f8fa0a28b" />
<br>
5. The code is written in Python and to process the data.
6. The code reads the data from the Pub/Sub topic, applies the transformation logic, and writes the transformed data to the BigQuery table.
7. The code is executed in a Dataflow job, which is a fully managed service for processing data in the cloud.
8. Click on the `Run Job` option.

<br>
<img width="917" alt="image" src="https://github.com/user-attachments/assets/b96935c8-8b72-487f-856b-859db8c0e6e7" />
<br>

<br>
<img width="918" alt="image" src="https://github.com/user-attachments/assets/f1339968-fe95-4469-9018-85646b3bac14" />
<br>

### DataFlow Job Details

**The graph View of Jobs**
<br>
<img width="1592" alt="image" src="https://github.com/user-attachments/assets/c19ad53c-3121-44a0-b89c-470886857191" />
<br>

The job is in continuous running state.

**The Table View of the Jobs**
<br>
<img width="1596" alt="image" src="https://github.com/user-attachments/assets/b47df810-0d64-4bb3-a0f3-0201cd07aaa9" />
<br>

## Results

1. Ran the python code.
<br>
<img width="1297" alt="image" src="https://github.com/user-attachments/assets/80fe0888-7eb8-4d2d-a34c-89f2c97d1b00" />
<br>

3. See the tables in Bigquery Console.
<br>
<img width="1297" alt="image" src="https://github.com/user-attachments/assets/e8fde1f3-caa1-4894-b52f-051a10312411" />
<br>

4. DataFlow logs
<br>
<img width="1107" alt="image" src="https://github.com/user-attachments/assets/9b9c693f-6a31-4562-8a69-a0b581993295" />
<br>


## FAQs

1. How to stop the job ?
<br>
<img width="1338" alt="image" src="https://github.com/user-attachments/assets/a338f960-ddf8-4e30-aedc-f250af9a5512" />
<br>

Choose `Drain` to gracefully stop the job. 

2. How to add gcp credentials to local environment ?
   - To to service account and cick on keys and click on `Add Keys`
     <img width="1090" alt="image" src="https://github.com/user-attachments/assets/c11dd86e-c49d-4540-ae63-fd941489d946" />
   - Use below code to add the keys.
     -   if bash
         `export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-key.json"`
     -   if python
         `import os
          from google.cloud import pubsub_v1
         
          os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/path/to/key.json"
         
          publisher = pubsub_v1.PublisherClient()
         `







